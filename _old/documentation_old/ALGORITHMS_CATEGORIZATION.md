# Categorization of Algorithms: Deep Learning vs Light ML

This document categorizes all algorithms used in the project based on their computational complexity and architecture type.

## üß† DEEP LEARNING Algorithms

### Neural Network-Based Algorithms

#### 1. **PPO (Proximal Policy Optimization) with MlpPolicy**
- **Location**: `RL_training.py:68`
- **Type**: Reinforcement Learning with Neural Network Policy
- **Architecture**: Multi-Layer Perceptron (MLP) - Deep Neural Network
- **Implementation**: `PPO("MlpPolicy", vec_env, verbose=1)`
- **Characteristics**:
  - Uses deep neural networks as policy/value functions
  - Multiple hidden layers (default PPO architecture)
  - Requires GPU for optimal performance (though can run on CPU)
  - Training: 10,000 timesteps
  - **Computational Cost**: HIGH ‚ö†Ô∏è
  - **Memory Usage**: HIGH ‚ö†Ô∏è

#### 2. **MLP (Multilayer Perceptrons)** - *Mentioned in README*
- **Location**: Mentioned in README.md (line 26, 47)
- **Type**: Deep Learning Classification Model
- **Status**: ‚ö†Ô∏è **Not in current code** (results shown but implementation not visible)
- **Performance**: Accuracy: 99.99% (from README results)
- **Characteristics**:
  - Multiple hidden layers (deep neural network)
  - Requires TensorFlow/Keras or PyTorch
  - **Computational Cost**: HIGH ‚ö†Ô∏è

#### 3. **DQN (Deep Q-Network)** - *Mentioned in README*
- **Location**: Mentioned in README.md (line 26)
- **Type**: Deep Reinforcement Learning
- **Status**: ‚ö†Ô∏è **Not in current code** (mentioned but not implemented)
- **Characteristics**:
  - Deep neural network for Q-value approximation
  - Requires significant computational resources
  - **Computational Cost**: HIGH ‚ö†Ô∏è

---

## üöÄ LIGHT / TRADITIONAL ML Algorithms

### These algorithms are fast, interpretable, and have low computational requirements.

#### 1. **Logistic Regression**
- **Location**: `data_training.py:144`
- **Type**: Linear Classification
- **Implementation**: `LogisticRegression(max_iter=1000)`
- **Library**: scikit-learn
- **Characteristics**:
  - Linear model with sigmoid activation
  - Fast training and prediction
  - Interpretable (feature coefficients)
  - **Computational Cost**: LOW ‚úÖ
  - **Memory Usage**: LOW ‚úÖ
  - **Training Time**: Seconds
  - **Accuracy**: 86.4%

#### 2. **Ridge Classifier**
- **Location**: `data_training.py:173`
- **Type**: Regularized Linear Classification
- **Implementation**: `RidgeClassifier()`
- **Library**: scikit-learn
- **Characteristics**:
  - L2 regularization (prevents overfitting)
  - Linear model
  - Very fast
  - **Computational Cost**: LOW ‚úÖ
  - **Memory Usage**: LOW ‚úÖ
  - **Training Time**: Seconds
  - **Accuracy**: 82.3%

#### 3. **Random Forest Classifier**
- **Location**: `data_training.py:145`
- **Type**: Ensemble Learning (Tree-based)
- **Implementation**: `RandomForestClassifier()`
- **Library**: scikit-learn
- **Characteristics**:
  - Multiple decision trees (ensemble)
  - Can be parallelized
  - Feature importance available
  - **Computational Cost**: MEDIUM ‚ö°
  - **Memory Usage**: MEDIUM ‚ö°
  - **Training Time**: Minutes (depending on data size)
  - **Accuracy**: 99.85% (Best performing!)

#### 4. **Gradient Boosting Classifier**
- **Location**: `data_training.py:146`
- **Type**: Ensemble Learning (Boosting)
- **Implementation**: `GradientBoostingClassifier()`
- **Library**: scikit-learn
- **Characteristics**:
  - Sequential tree building (boosting)
  - More accurate than single trees
  - Slower than Random Forest
  - **Computational Cost**: MEDIUM ‚ö°
  - **Memory Usage**: MEDIUM ‚ö°
  - **Training Time**: Minutes to hours
  - **Accuracy**: 99.34%

#### 5. **XGBoost Classifier**
- **Location**: `data_training.py:197`
- **Type**: Advanced Gradient Boosting
- **Implementation**: `XGBClassifier(use_label_encoder=False, eval_metric='logloss')`
- **Library**: XGBoost
- **Characteristics**:
  - Optimized gradient boosting
  - Handles missing values
  - Can use GPU acceleration (optional)
  - **Computational Cost**: MEDIUM-HIGH ‚ö°‚ö†Ô∏è
  - **Memory Usage**: MEDIUM ‚ö°
  - **Training Time**: Minutes
  - **Accuracy**: 99.85% (Best performing alongside Random Forest!)

#### 6. **SVM (Support Vector Machine)**
- **Location**: `data_training.py:137` (imported but **NOT USED**)
- **Type**: Kernel-based Classification
- **Status**: ‚ö†Ô∏è **Imported but not implemented in current code**
- **Characteristics**:
  - Can be slow for large datasets
  - Good for non-linear problems (with kernels)
  - **Computational Cost**: MEDIUM-HIGH ‚ö°‚ö†Ô∏è

---

## üìä Summary Table

| Algorithm | Type | Computational Cost | Memory | Training Time | Accuracy | In Code? |
|-----------|------|-------------------|--------|---------------|----------|----------|
| **DEEP LEARNING** | | | | | | |
| PPO (MlpPolicy) | RL + Neural Net | üî¥ HIGH | üî¥ HIGH | Hours | Variable | ‚úÖ Yes |
| MLP | Neural Network | üî¥ HIGH | üî¥ HIGH | Hours | 99.99% | ‚ùå No* |
| DQN | Deep RL | üî¥ HIGH | üî¥ HIGH | Hours | Variable | ‚ùå No* |
| **LIGHT ML** | | | | | | |
| Logistic Regression | Linear | üü¢ LOW | üü¢ LOW | Seconds | 86.4% | ‚úÖ Yes |
| Ridge Classifier | Linear | üü¢ LOW | üü¢ LOW | Seconds | 82.3% | ‚úÖ Yes |
| Random Forest | Ensemble | üü° MEDIUM | üü° MEDIUM | Minutes | **99.85%** | ‚úÖ Yes |
| Gradient Boosting | Ensemble | üü° MEDIUM | üü° MEDIUM | Minutes | 99.34% | ‚úÖ Yes |
| XGBoost | Advanced Ensemble | üü° MEDIUM | üü° MEDIUM | Minutes | **99.85%** | ‚úÖ Yes |
| SVM | Kernel-based | üü° MEDIUM | üü° MEDIUM | Minutes-Hours | - | ‚ùå Imported only |

\* *Mentioned in README/results but implementation not in current code files*

---

## üéØ Recommendations

### For Fast Inference / Production:
- ‚úÖ **Random Forest** or **XGBoost** - Best accuracy (99.85%) with reasonable speed
- ‚úÖ **Logistic Regression** - Fastest, acceptable accuracy (86.4%)

### For Maximum Accuracy:
- ‚úÖ **MLP** (if implemented) - 99.99% accuracy (requires GPU)
- ‚úÖ **Random Forest / XGBoost** - 99.85% accuracy (CPU-friendly)

### For Reinforcement Learning:
- ‚úÖ **PPO with MlpPolicy** - Deep RL approach (requires more resources)

### Avoid for Large-Scale Production:
- ‚ö†Ô∏è **PPO** - Too slow for real-time inference
- ‚ö†Ô∏è **Deep Learning models** - Require GPU and more memory

---

## üìà Performance vs Computational Cost Comparison

```
Accuracy
100% |                                    MLP (99.99%)
     |                    RF/XGBoost (99.85%)
     |                    GB (99.34%)
     |
 80% |        LR (86.4%)
     |    Ridge (82.3%)
     |
     |___|___|___|___|___|___|___|___|___|___|___|___|___|___|___
     LOW      MEDIUM    HIGH        Computational Cost

Legend:
‚úÖ Green: Light algorithms (fast, CPU-friendly)
‚ö° Yellow: Medium complexity (parallelizable, optimized)
üî¥ Red: Deep learning (requires GPU, high memory)
```

---

## üîç Key Insights

1. **Best Light Algorithm**: Random Forest and XGBoost achieve **99.85% accuracy** - competitive with deep learning!
2. **Fastest Algorithm**: Logistic Regression and Ridge (seconds)
3. **Deep Learning**: Only PPO is implemented; MLP/DQN mentioned but code not present
4. **Production-Ready**: Random Forest or XGBoost offer best balance of accuracy and speed
5. **Trade-off**: Deep learning models (MLP) achieve slightly better accuracy (99.99% vs 99.85%) but require significantly more resources
