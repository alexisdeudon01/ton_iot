# Test Coverage Documentation

**Last Updated**: 2024-01-18

## ğŸ“‹ Vue d'ensemble

Le fichier `main_test.py` est un runner de tests unifiÃ© qui exÃ©cute tous les tests pytest du projet avec:
- **Logs dÃ©taillÃ©s** : input/output Ã  chaque Ã©tape
- **Explications** : pourquoi chaque test rÃ©ussit/Ã©choue
- **Diagrammes** : gÃ©nÃ©rÃ©s dans `output/test_reports/test_run_TIMESTAMP/`
- **Rapports** : Markdown dÃ©taillÃ© et JSON avec tous les rÃ©sultats

## ğŸ¯ Couverture des Tests

### 1. Tests de Preprocessing Pipeline

**Fichiers testÃ©s** : `tests/test_preprocessing_pipeline.py`

**Couverture** :
- âœ… Preprocessing stateless (nettoyage, encodage)
- âœ… Transform methods (`transform_test()`, `transform_data()`)
- âœ… ZÃ©ro data leakage (scaler/selector fittÃ© uniquement sur TRAIN)
- âœ… Sanitization numÃ©rique (inf, outliers)
- âœ… Validation pipeline fitted state

**Exemples de tests** :
- `test_sanitize_numeric_values_removes_inf_and_clips`
- `test_transform_test_requires_fitted_pipeline`
- `test_no_data_leakage_scaler_fit_only_on_train`

---

### 2. Tests Phase 2 (Apply Best Configuration)

**Fichiers testÃ©s** : `tests/test_phase2_outputs.py`

**Couverture** :
- âœ… GÃ©nÃ©ration de `best_preprocessed.parquet` (ou `.csv.gz`)
- âœ… GÃ©nÃ©ration de `feature_names.json`
- âœ… GÃ©nÃ©ration de `phase2_summary.md`
- âœ… PrÃ©sence et encodage correct de `dataset_source` (0/1)
- âœ… Preprocessing stateless uniquement (pas de scaling/FS/SMOTE)

**Input** : Mock config et dataset avec `dataset_source`

**Expected Output** :
- Fichier preprocessed avec `dataset_source` encodÃ©
- JSON avec liste des features
- Summary Markdown avec statistiques

---

### 3. Tests Phase 3 (3D Evaluation)

**Fichiers testÃ©s** : 
- `tests/test_phase3_synthetic.py`
- `tests/test_phase3_cnn_tabnet.py`

**Couverture** :
- âœ… Mode synthÃ©tique (`--synthetic` flag)
- âœ… Preprocessing model-aware par fold
- âœ… GÃ©nÃ©ration de CSV (evaluation_results.csv, dimension_scores.csv)
- âœ… GÃ©nÃ©ration de rapports algorithmes (algorithm_reports/*.md)
- âœ… GÃ©nÃ©ration de visualisations (visualizations/*.png)
- âœ… Support CNN/TabNet avec reshape et class_weight

**Input** : Config avec `synthetic_mode=True` ou datasets rÃ©els

**Expected Output** :
- CSV avec mÃ©triques (F1, accuracy, precision, recall, temps, mÃ©moire, explainability)
- Rapports Markdown par algorithme
- Diagrammes de visualisation
- INDEX.md pour mÃ©triques et visualisations

---

### 4. Tests Model-Aware Preprocessing Profiles

**Fichiers testÃ©s** : `tests/test_model_aware_profiles.py`

**Couverture** :
- âœ… Profil LR : scaling=True, feature_selection=True, resampling=True
- âœ… Profil Tree (DT/RF) : scaling=False, feature_selection=False, class_weight='balanced'
- âœ… Profil CNN : scaling=True, feature_selection=False, resampling=True, cnn_reshape=True
- âœ… Profil TabNet : scaling=False, feature_selection=False, class_weight='balanced'
- âœ… Calcul dynamique de `feature_selection_k`

**Input** : Noms de modÃ¨les (LR, DT, RF, CNN, TabNet)

**Expected Output** : Profiles avec bonnes valeurs boolÃ©ennes et paramÃ¨tres

---

### 5. Tests ZÃ©ro Data Leakage

**Fichiers testÃ©s** : `tests/test_no_data_leakage.py`

**Couverture** :
- âœ… Scaler fittÃ© uniquement sur TRAIN fold
- âœ… Test transformÃ© avec scaler fittÃ© sur TRAIN
- âœ… Feature selector fittÃ© uniquement sur TRAIN
- âœ… Test transformÃ© avec selector fittÃ© sur TRAIN
- âœ… Imputer fittÃ© uniquement sur TRAIN (mÃ©diane calculÃ©e sur TRAIN)

**Input** : TRAIN et TEST DataFrames avec distributions diffÃ©rentes

**Expected Output** : TEST transformÃ© avec statistiques (moyenne, mÃ©diane) basÃ©es sur TRAIN uniquement

---

### 6. Tests Algorithm Handling

**Fichiers testÃ©s** : `tests/test_algo_handling.py`

**Couverture** :
- âœ… `get_algo_names()` : extraction des noms d'algorithmes depuis DataFrame
- âœ… `ensure_algo_column()` : crÃ©ation de colonne 'algo' si manquante
- âœ… `sanitize_algo_name()` : nettoyage des noms pour fichiers (espaces â†’ underscore)

**Input** : DataFrames avec colonne 'algo' ou index nommÃ© 'algo'

**Expected Output** : Series avec noms d'algorithmes normalisÃ©s

---

### 7. Tests Dataset Source Flag

**Fichiers testÃ©s** : `tests/test_dataset_source_flag.py`

**Couverture** :
- âœ… Flag `phase3_use_dataset_source` contrÃ´le inclusion de `dataset_source`
- âœ… `dataset_source` prÃ©servÃ© quand flag=True
- âœ… `dataset_source` exclu quand flag=False

**Input** : Config avec flag True/False, DataFrame avec `dataset_source`

**Expected Output** : Features avec/sans `dataset_source` selon flag

---

### 8. Tests CNN/TabNet

**Fichiers testÃ©s** : 
- `tests/test_cnn.py`
- `tests/test_tabnet.py`

**Couverture** :
- âœ… Initialisation CNN/TabNet
- âœ… Training sur donnÃ©es synthÃ©tiques
- âœ… Prediction et predict_proba
- âœ… Validation sklearn interface (fit/predict/predict_proba)
- âœ… Reshape CNN pour input (n, d, 1)
- âœ… Class_weight pour TabNet

**Input** : DonnÃ©es synthÃ©tiques (X, y) binaires/multiclass

**Expected Output** : ModÃ¨le entraÃ®nÃ© avec prÃ©dictions valides

---

## ğŸ“Š Structure des Rapports GÃ©nÃ©rÃ©s

### RÃ©pertoire de sortie
```
output/test_reports/test_run_YYYYMMDD_HHMMSS/
â”œâ”€â”€ test_coverage_diagram.png    # Diagramme de couverture par catÃ©gorie
â”œâ”€â”€ test_report.md               # Rapport Markdown dÃ©taillÃ©
â””â”€â”€ test_results.json            # RÃ©sultats JSON complets
```

### Contenu du rapport Markdown

1. **Summary Statistics** :
   - Total de tests
   - Passed/Failed/Skipped avec pourcentages

2. **Test Results by Outcome** :
   - âœ… **Passed Tests** : nom, durÃ©e, input, output, raison du succÃ¨s
   - âŒ **Failed Tests** : nom, durÃ©e, input, raison de l'Ã©chec, message d'erreur, traceback
   - â­ï¸ **Skipped Tests** : nom, raison du skip

### Contenu du JSON

```json
{
  "timestamp": "2024-01-18T10:30:00",
  "total_tests": 29,
  "passed": 25,
  "failed": 2,
  "skipped": 2,
  "results": [
    {
      "test_name": "tests/test_phase2_outputs.py::test_phase2_outputs",
      "outcome": "passed",
      "duration": 1.234,
      "input_description": "Mock config and dataset...",
      "output_description": "Preprocessed data file...",
      "success_reason": "Phase 2 outputs successfully generated...",
      ...
    }
  ]
}
```

## ğŸ¨ Diagrammes GÃ©nÃ©rÃ©s

### Test Coverage Diagram

1. **Graphique en barres** (gauche) :
   - Nombre de tests Passed/Failed/Skipped par catÃ©gorie :
     - Preprocessing
     - Phase 2
     - Phase 3
     - Models (CNN/TabNet/sklearn)
     - Data Leakage
     - Algorithm Handling
     - Dataset Source
     - Model Profiles
     - Other

2. **Graphique en camembert** (droite) :
   - Distribution globale : Passed vs Failed vs Skipped
   - Pourcentages et totaux

## ğŸ” Explications de SuccÃ¨s/Ã‰chec

### Raisons de succÃ¨s automatiques

Le plugin gÃ©nÃ¨re automatiquement des explications selon le type de test:

- **Preprocessing tests** : "Preprocessing pipeline correctly applied stateless transformations and maintained data integrity"
- **Phase 2 tests** : "Phase 2 outputs successfully generated with correct format"
- **Phase 3 tests** : "Phase 3 evaluation completed with model-aware preprocessing per fold, ensuring zero data leakage"
- **CNN tests** : "CNN model correctly initialized, trained, and evaluated with proper input reshaping"
- **Leakage tests** : "No data leakage detected: scaler/selector fitted only on TRAIN, test transformed using TRAIN-fitted objects"

### Raisons d'Ã©chec automatiques

- **AssertionError** : "Assertion failed: [message]"
- **ValueError** : "Invalid value: [message]"
- **AttributeError** : "Missing attribute: [message]"
- **KeyError** : "Missing key: [message]"
- **ImportError** : "Missing dependency: [module]"
- **FileNotFoundError** : "File not found: [path]"

## ğŸ“ Utilisation

### Lancer tous les tests
```bash
python main_test.py
```

### Lancer avec verbose
```bash
python main_test.py -v
```

### Lancer un test spÃ©cifique
```bash
pytest tests/test_phase2_outputs.py::test_phase2_outputs -v
```

## ğŸ“ˆ Statistiques Typiques

- **Total de tests** : ~29 fichiers de tests
- **Tests actifs** : ~20-25 (selon dÃ©pendances)
- **Temps d'exÃ©cution** : 30-120 secondes selon environnement
- **Couverture estimÃ©e** :
  - Preprocessing : ~100%
  - Phase 2 : ~100%
  - Phase 3 : ~90% (CNN/TabNet optionnels)
  - Models : ~85% (selon dÃ©pendances)

## ğŸ”§ DÃ©pendances pour Tests Complets

**Requis** :
- pytest
- pandas
- numpy
- scikit-learn

**Optionnels** (pour certains tests) :
- torch (CNN tests)
- pytorch-tabnet (TabNet tests)
- shap (explainability tests)
- lime (explainability tests)
- matplotlib (diagram generation)
