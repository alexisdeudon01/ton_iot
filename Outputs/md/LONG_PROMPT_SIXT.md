# PROMPT COMPLET - TON IoT ML Pipeline DDoS Detection

## üéØ CONTEXTE DU PROJET

Tu es un expert en Machine Learning et architecture logicielle travaillant sur un pipeline de d√©tection DDoS pour l'IoT. Le projet analyse deux datasets majeurs (ToN-IoT et CIC-DDoS2019) avec plusieurs algorithmes ML (LR, DT, RF, KNN, CNN, TabNet).

### Architecture Actuelle

```
src/new_pipeline/
‚îú‚îÄ‚îÄ data_loader.py (188 lignes) - Chargement Dask lazy
‚îú‚îÄ‚îÄ trainer.py (187 lignes) - Entra√Ænement 6 mod√®les
‚îú‚îÄ‚îÄ validator.py (98 lignes) - Tuning hyperparam√®tres
‚îú‚îÄ‚îÄ tester.py (127 lignes) - √âvaluation finale
‚îú‚îÄ‚îÄ xai_manager.py (132 lignes) - Explainabilit√© (SHAP, LIME)
‚îú‚îÄ‚îÄ main.py (233 lignes) - Orchestration
‚îî‚îÄ‚îÄ config.py (52 lignes) - Configuration

Total: ~1017 lignes
```

### Technologies Utilis√©es

- **Dask**: Pour processing out-of-core (datasets 70M+ lignes)
- **PyTorch**: Pour CNN personnalis√© sur donn√©es tabulaires
- **TabNet**: R√©seau attentionnel pour donn√©es tabulaires
- **Scikit-learn**: Mod√®les traditionnels (LR, DT, RF, KNN)
- **SHAP/LIME**: Pour explainabilit√© mod√®les
- **SystemMonitor**: Thread background monitoring CPU/RAM

---

## üî¥ PROBL√àMES CRITIQUES IDENTIFI√âS

### CRITIQUE #1: Conversions Dask‚ÜíPandas Non S√©curis√©es

**Localisation**: `trainer.py:70-80`, `validator.py:27-30`, `tester.py:25-28`, `xai_manager.py:35-38`

**Probl√®me**:

```python
# Code actuel - DANGEREUX
if isinstance(X_train, dd.DataFrame):
    X_train_pd = X_train.head(100000)  # ‚ö†Ô∏è Pas de v√©rification RAM!
    y_train_pd = y_train.head(100000)
```

**Impact**:

- Avec 50 colonnes √ó 100k lignes √ó 8 bytes = 40+ MB
- Peut exploser avec colonnes object/string
- **Cause des warnings RAM √† 93%** visibles dans les logs
- R√©p√©t√© 3 fois (code dupliqu√©)

**Solution Requise**: Cr√©er `MemoryAwareProcessor` qui:

1. Estime la taille m√©moire du DataFrame
2. V√©rifie la RAM disponible
3. Calcule un sample size s√ªr (‚â§70% RAM disponible)
4. Log toutes les d√©cisions
5. Remplace les 4 occurrences de head()

---

### CRITIQUE #2: Gestion d'Erreurs Basique

**Localisation**: Partout dans le code

**Probl√®me**:

```python
try:
    # ... code ...
except Exception as e:  # ‚ö†Ô∏è Trop large, masque les bugs
    logger.error(f"Erreur: {e}")  # Pas de retry, pas de contexte
```

**Occurrences**:

- `trainer.py:109` - Entra√Ænement √©choue silencieusement
- `validator.py:69` - Tuning √©choue silencieusement
- `tester.py:70` - √âvaluation √©choue silencieusement
- `xai_manager.py:130` - XAI √©choue silencieusement

**Impact**:

- Erreurs temporaires non r√©cup√©r√©es
- Bugs masqu√©s
- Debugging difficile

**Solution Requise**: Framework d'exceptions avec:

- Exceptions personnalis√©es typ√©es
- Distinction erreurs recouvrables/critiques
- Retry automatique avec backoff exponentiel
- Contexte d√©taill√© (model, phase, donn√©es)

---

### MOYEN #3: Retours de Fonctions Absents

**Probl√®me**:

```python
def train_single(self, name, X_train, y_train):
    # ... train model ...
    # ‚ùå Pas de return - r√©sultats dans self.models

def evaluate_all(self, X_test, y_test, algo_name=None):
    # ... evaluate ...
    # ‚ùå R√©sultats dans self.test_results
```

**Impact**:

- Impossible √† tester unitairement
- √âtat mut√© au lieu de retours explicites
- Pas de type hints
- Cha√Ænage impossible

**Solution Requise**: Result Objects (dataclasses) avec:

- `TrainingResult`: model_name, success, time, history, error
- `ValidationResult`: model_name, best_params, scores
- `TestResult`: model_name, metrics (accuracy, f1, precision, recall, auc)
- Type hints explicites
- M√©thodes helper (to_dict, from_dict)

---

### MINEUR #4: Visualisation M√©lang√©e avec Logique

**Localisation**: `trainer.py:147-180` (40+ lignes matplotlib)

**Probl√®me**: Classe Trainer fait trop de choses (violation Single Responsibility)

**Solution Requise**: `VisualizationService` centralis√© avec m√©thodes:

- `plot_training_times(times: Dict[str, float])`
- `plot_convergence(name: str, history: Dict)`
- `plot_metrics_comparison(results: Dict)`
- `plot_confusion_matrices(cms: Dict)`
- `plot_resource_usage(monitor: SystemMonitor)`

---

### MINEUR #5: Configuration Sans Validation

**Localisation**: `config.py` (dicts simples)

**Probl√®me**: Pas de validation, typos possibles, pas de types

**Solution Requise**: Pydantic Config avec:

- Validation automatique des paths
- Validation des plages (memory 10-90%)
- Type hints stricts
- M√©thode `from_yaml()`
- Auto-compl√©tion IDE

---

## üéØ PLAN D'ACTION PRIORITAIRE (2 SEMAINES)

### SEMAINE 1: FIXES CRITIQUES

**Jour 1 (AUJOURD'HUI) - MemoryAwareProcessor**

```python
# Cr√©er src/core/memory_manager.py
class MemoryAwareProcessor:
    def __init__(self, safety_margin: float = 0.7):
        self.safety_margin = safety_margin

    def safe_compute(self, dask_df: dd.DataFrame,
                     operation: str) -> pd.DataFrame:
        # 1. Estimer taille: n_rows * n_cols * 8 * 1.2
        # 2. V√©rifier RAM disponible
        # 3. Si trop grand: sample avec ratio s√ªr
        # 4. Logger d√©cision
        # 5. Retourner DataFrame pandas
        pass
```

**Int√©gration**:

- `trainer.py`: Remplacer lignes 70-74
- `validator.py`: Remplacer lignes 27-30
- `tester.py`: Remplacer lignes 25-28
- `xai_manager.py`: Remplacer lignes 35-38

**Tests**:

- `python3 main.py --test-mode` (0.1% data)
- V√©rifier logs: "[MemoryAware] RAM suffisante, compute() complet"
- `python3 main.py --sample-ratio 0.5` (50% data)
- V√©rifier RAM reste < 70%

---

**Jour 2 - Framework Exceptions**

```python
# Cr√©er src/core/exceptions.py
class PipelineException(Exception):
    def __init__(self, message: str, details: dict = None):
        self.message = message
        self.details = details or {}

class ModelTrainingError(PipelineException): pass
class InsufficientMemoryError(PipelineException): pass
class DataLoadingError(PipelineException): pass
class ConfigurationError(PipelineException): pass
```

**Int√©gration**:

- Remplacer tous les `except Exception as e`
- Ajouter contexte dans details
- Propager erreurs critiques (raise)
- Logger erreurs avec extra=details

---

**Jour 3 - Result Objects**

```python
# Cr√©er src/core/results.py
@dataclass
class TrainingResult:
    model_name: str
    success: bool
    training_time: float
    history: Dict[str, List[float]]
    error_message: Optional[str] = None

    @property
    def final_loss(self) -> float: ...
    @property
    def final_accuracy(self) -> float: ...

# Idem pour ValidationResult, TestResult, XAIResult
```

**Int√©gration**:

- `trainer.py`: `def train_single(...) -> TrainingResult`
- `validator.py`: `def validate_tuning(...) -> ValidationResult`
- `tester.py`: `def evaluate_all(...) -> Dict[str, TestResult]`
- Ajouter type hints partout

---

**Jour 4 - Tests & Validation**

- Tester pipeline complet end-to-end
- V√©rifier RAM < 70% en continu
- V√©rifier tous les Result objects
- V√©rifier toutes les exceptions typ√©es
- Benchmarker: temps, RAM peak, pr√©cision

---

**Jour 5 - Documentation**

- Mettre √† jour README.md
- Documenter API (docstrings)
- Cr√©er guide migration
- Ajouter exemples d'utilisation

---

### SEMAINE 2: REFACTORING ARCHITECTURE

**Jours 6-7 - VisualizationService**

```python
# Cr√©er src/evaluation/visualization_service.py
class VisualizationService:
    def __init__(self, output_dir: Path):
        self.output_dir = output_dir
        plt.style.use('seaborn-v0_8')

    def plot_training_times(self, times: Dict[str, float]) -> Path: ...
    def plot_convergence(self, name: str, history: Dict) -> Path: ...
    def plot_metrics_comparison(self, results: Dict) -> Path: ...
```

**Int√©gration**:

- Extraire code matplotlib de `trainer.py` (lignes 147-180)
- Extraire de `validator.py`, `tester.py`, `xai_manager.py`
- Injecter `VisualizationService` dans constructeurs
- Supprimer code dupliqu√©

---

**Jours 8-9 - Pydantic Configuration**

```python
# Modifier src/new_pipeline/config.py
from pydantic import BaseModel, Field, validator

class PipelineConfig(BaseModel):
    # Paths
    ton_iot_path: Path = Field(...)
    cic_ddos_dir: Path = Field(...)

    # Algorithms
    algorithms: List[Literal['LR', 'DT', 'RF', 'KNN', 'CNN', 'TabNet']]

    # Resources
    max_memory_percent: float = Field(50.0, ge=10.0, le=90.0)

    @validator('ton_iot_path')
    def validate_path_exists(cls, v):
        if not v.exists():
            raise ValueError(f"Path not found: {v}")
        return v

    class Config:
        validate_assignment = True
```

**Int√©gration**:

- Remplacer dicts par PipelineConfig
- Mettre √† jour `main.py`
- Cr√©er `config.yaml` exemple
- Ajouter `config = PipelineConfig.from_yaml("config.yaml")`

---

**Jour 10 - Tests Finaux & Release**

- Tests d'int√©gration complets
- Tests avec full dataset (--sample-ratio 1.0)
- Monitoring RAM/CPU pendant 1h
- Cr√©er release notes
- Git tag v2.0.0

---

## üìã CODE TEMPLATES PR√äTS √Ä UTILISER

### 1. MemoryAwareProcessor (PRIORIT√â #1)

```python
# src/core/memory_manager.py
import psutil
import logging
import dask.dataframe as dd
import pandas as pd
from typing import Optional

logger = logging.getLogger(__name__)

class MemoryAwareProcessor:
    """Convertit intelligemment Dask‚ÜíPandas selon RAM disponible"""

    def __init__(self, safety_margin: float = 0.7):
        """
        Args:
            safety_margin: Pourcentage RAM disponible √† utiliser (0.7 = 70%)
        """
        self.safety_margin = safety_margin
        logger.info(f"[MemoryAware] Initialis√© avec safety_margin={safety_margin*100:.0f}%")

    def safe_compute(self, dask_df: dd.DataFrame,
                     operation: str = "unknown") -> pd.DataFrame:
        """
        Convertit Dask‚ÜíPandas en respectant les limites RAM

        Args:
            dask_df: DataFrame Dask √† convertir
            operation: Nom de l'op√©ration (pour logging)

        Returns:
            DataFrame pandas (complet ou sampl√©)
        """
        # 1. Estimer taille en m√©moire
        n_rows = len(dask_df)
        n_cols = len(dask_df.columns)

        # Estimation: 8 bytes/val num√©rique + 20% overhead
        estimated_bytes = n_rows * n_cols * 8 * 1.2
        estimated_mb = estimated_bytes / (1024 * 1024)

        # 2. V√©rifier RAM disponible
        mem = psutil.virtual_memory()
        available_mb = mem.available / (1024 * 1024)
        safe_mb = available_mb * self.safety_margin

        logger.info(
            f"[MemoryAware] {operation}: "
            f"Estim√©={estimated_mb:.1f}MB, "
            f"Disponible={available_mb:.1f}MB, "
            f"Safe={safe_mb:.1f}MB, "
            f"RAM actuelle={mem.percent:.1f}%"
        )

        # 3. D√©cision: compute ou sample
        if estimated_mb <= safe_mb:
            logger.info(f"[MemoryAware] ‚úì RAM suffisante, compute() complet")
            return dask_df.compute()
        else:
            # Calculer ratio s√ªr
            safe_ratio = safe_mb / estimated_mb
            safe_rows = int(n_rows * safe_ratio)

            logger.warning(
                f"[MemoryAware] ‚ö† RAM insuffisante! "
                f"Sampling {safe_rows:,} rows ({safe_ratio*100:.1f}%) "
                f"au lieu de {n_rows:,}"
            )

            # √âchantillonnage stratifi√© si colonne target pr√©sente
            if "is_ddos" in dask_df.columns:
                return dask_df.sample(frac=safe_ratio, random_state=42).compute()
            else:
                return dask_df.head(safe_rows)

    def get_memory_status(self) -> dict:
        """Retourne √©tat RAM actuel"""
        mem = psutil.virtual_memory()
        return {
            'total_gb': mem.total / (1024**3),
            'available_gb': mem.available / (1024**3),
            'percent_used': mem.percent,
            'safe_available_gb': (mem.available * self.safety_margin) / (1024**3)
        }

    def estimate_dataframe_size(self, n_rows: int, n_cols: int) -> float:
        """Estime taille DataFrame en MB"""
        return (n_rows * n_cols * 8 * 1.2) / (1024 * 1024)
```

**Int√©gration dans trainer.py**:

```python
# En haut du fichier
from src.core.memory_manager import MemoryAwareProcessor

class PipelineTrainer:
    def __init__(self, random_state=42):
        self.random_state = random_state
        self.memory_mgr = MemoryAwareProcessor(safety_margin=0.7)  # ‚úÖ AJOUT
        self.models = {...}
        self.history = {}
        self.training_times = {}

    def train_single(self, name, X_train, y_train):
        """Trains a single model by name, handling Dask dataframes."""

        # REMPLACER ces lignes:
        # if isinstance(X_train, dd.DataFrame):
        #     X_train_pd = X_train.head(100000)
        #     y_train_pd = y_train.head(100000)

        # PAR:
        if isinstance(X_train, dd.DataFrame):
            X_train_pd = self.memory_mgr.safe_compute(X_train, f"training_{name}_X")
            y_train_pd = self.memory_mgr.safe_compute(y_train, f"training_{name}_y")
        else:
            X_train_pd = X_train
            y_train_pd = y_train

        # ... reste du code identique ...
```

---

### 2. Framework Exceptions

```python
# src/core/exceptions.py
"""Exceptions personnalis√©es pour le pipeline ML"""

class PipelineException(Exception):
    """Exception de base pour tout le pipeline"""
    def __init__(self, message: str, details: dict = None):
        self.message = message
        self.details = details or {}
        super().__init__(self.message)

    def __str__(self):
        if self.details:
            details_str = ", ".join(f"{k}={v}" for k, v in self.details.items())
            return f"{self.message} [{details_str}]"
        return self.message

class DataLoadingError(PipelineException):
    """Erreur lors du chargement des donn√©es"""
    pass

class InsufficientMemoryError(PipelineException):
    """RAM insuffisante pour l'op√©ration"""
    def __init__(self, required_mb: float, available_mb: float):
        super().__init__(
            f"RAM insuffisante: besoin {required_mb:.1f}MB, disponible {available_mb:.1f}MB",
            details={
                'required_mb': required_mb,
                'available_mb': available_mb,
                'deficit_mb': required_mb - available_mb
            }
        )

class ModelTrainingError(PipelineException):
    """Erreur lors de l'entra√Ænement d'un mod√®le"""
    def __init__(self, model_name: str, original_error: Exception):
        super().__init__(
            f"√âchec entra√Ænement {model_name}: {str(original_error)}",
            details={
                'model': model_name,
                'error_type': type(original_error).__name__,
                'original_message': str(original_error)
            }
        )

class ValidationError(PipelineException):
    """Erreur lors de la validation"""
    pass

class ConfigurationError(PipelineException):
    """Erreur de configuration"""
    pass

class XAIError(PipelineException):
    """Erreur lors de l'analyse XAI"""
    pass
```

**Int√©gration dans trainer.py**:

```python
from src.core.exceptions import ModelTrainingError, InsufficientMemoryError

def train_single(self, name, X_train, y_train):
    start_time = time.time()

    try:
        # ... code entra√Ænement ...

        self.training_times[name] = time.time() - start_time
        logger.info(f"{name} entra√Æn√© en {self.training_times[name]:.2f}s")

    except MemoryError as e:
        error = InsufficientMemoryError(
            required_mb=100,  # Estimation
            available_mb=psutil.virtual_memory().available / (1024**2)
        )
        logger.error(str(error), extra=error.details)
        raise error

    except Exception as e:
        error = ModelTrainingError(name, e)
        logger.error(str(error), extra=error.details)
        self.training_times[name] = 0
        raise error
```

---

### 3. Result Objects

```python
# src/core/results.py
"""Objets de r√©sultats structur√©s pour le pipeline"""
from dataclasses import dataclass, field, asdict
from typing import Dict, Optional, List, Any
import numpy as np
import json

@dataclass
class TrainingResult:
    """R√©sultat d'entra√Ænement d'un mod√®le"""
    model_name: str
    success: bool
    training_time: float
    history: Dict[str, List[float]] = field(default_factory=dict)
    error_message: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

    @property
    def final_loss(self) -> float:
        """Derni√®re valeur de loss"""
        return self.history.get('loss', [float('inf')])[-1]

    @property
    def final_accuracy(self) -> float:
        """Derni√®re accuracy"""
        return self.history.get('accuracy', [0.0])[-1]

    def to_dict(self) -> dict:
        """Convertit en dict pour s√©rialisation"""
        return asdict(self)

    def to_json(self) -> str:
        """Convertit en JSON"""
        return json.dumps(self.to_dict(), indent=2)

@dataclass
class ValidationResult:
    """R√©sultat de validation hyperparam√®tres"""
    model_name: str
    best_params: Dict[str, Any]
    best_score: float
    all_scores: Dict[str, float] = field(default_factory=dict)
    validation_time: float = 0.0

    @property
    def improvement_percent(self) -> float:
        """Am√©lioration en % vs pire config"""
        if not self.all_scores:
            return 0.0
        worst = min(self.all_scores.values())
        return ((self.best_score - worst) / worst * 100) if worst > 0 else 0.0

    def to_dict(self) -> dict:
        return asdict(self)

@dataclass
class TestResult:
    """R√©sultat d'√©valuation finale"""
    model_name: str
    accuracy: float
    f1_score: float
    precision: float
    recall: float
    auc: float
    confusion_matrix: Optional[np.ndarray] = None
    test_time: float = 0.0

    def to_dict(self) -> dict:
        """Convertit en dict (sans confusion_matrix pour JSON)"""
        result = {
            'model': self.model_name,
            'accuracy': float(self.accuracy),
            'f1_score': float(self.f1_score),
            'precision': float(self.precision),
            'recall': float(self.recall),
            'auc': float(self.auc),
            'test_time': float(self.test_time)
        }
        if self.confusion_matrix is not None:
            result['confusion_matrix'] = self.confusion_matrix.tolist()
        return result

    @property
    def overall_score(self) -> float:
        """Score global (moyenne des m√©triques)"""
        return (self.accuracy + self.f1_score + self.precision +
                self.recall + self.auc) / 5

@dataclass
class XAIResult:
    """R√©sultat d'analyse XAI"""
    model_name: str
    method: str
    fidelity: float
    stability: float
    complexity: float
    composite_score: float = 0.0

    def __post_init__(self):
        """Calcule composite_score si non fourni"""
        if self.composite_score == 0.0:
            self.composite_score = (
                self.fidelity * 0.4 +
                self.stability * 0.4 +
                self.complexity * 0.2
            )

    def to_dict(self) -> dict:
        return asdict(self)
```

**Int√©gration dans trainer.py**:

```python
from src.core.results import TrainingResult

def train_single(self, name, X_train, y_train) -> TrainingResult:  # ‚úÖ Type hint
    start_time = time.time()

    try:
        # ... code entra√Ænement ...

        return TrainingResult(
            model_name=name,
            success=True,
            training_time=time.time() - start_time,
            history=self.history.get(name, {}),
            metadata={'n_samples': len(X_train_num)}
        )

    except Exception as e:
        return TrainingResult(
            model_name=name,
            success=False,
            training_time=time.time() - start_time,
            error_message=str(e)
        )
```

---

## üß™ TESTS √Ä EX√âCUTER

### Test 1: Petit Dataset (0.1%)

```bash
python3 main.py --test-mode
# V√©rifier:
# - Pas d'erreur
# - RAM < 50%
# - Logs "[MemoryAware] ‚úì RAM suffisante"
```

### Test 2: Dataset Moyen (50%)

```bash
python3 main.py --sample-ratio 0.5
# V√©rifier:
# - RAM < 70%
# - Possibles "[MemoryAware] ‚ö† Sampling"
# - Temps < 30min
```

### Test 3: Dataset Complet (100%)

```bash
python3 main.py --sample-ratio 1.0
# V√©rifier:
# - Pas OOM
# - RAM < 80%
# - Logs sampling pour phases m√©moire-intensives
```

### Test 4: Erreurs & Recovery

```bash
# Supprimer temporairement un fichier dataset
# V√©rifier exception DataLoadingError lev√©e
# V√©rifier logs contexte d√©taill√©
```

---

## üìä M√âTRIQUES DE SUCC√àS

### Apr√®s Semaine 1

- ‚úÖ Z√©ro OOM sur 5 runs cons√©cutifs
- ‚úÖ RAM reste < 70% durant tout le pipeline
- ‚úÖ Toutes les exceptions typ√©es (aucun `except Exception`)
- ‚úÖ Toutes les fonctions retournent des Result objects
- ‚úÖ Logs explicites: "[MemoryAware] ..." visible partout

### Apr√®s Semaine 2

- ‚úÖ Code matplotlib s√©par√© (VisualizationService)
- ‚úÖ Configuration Pydantic valid√©e
- ‚úÖ Couverture tests > 60%
- ‚úÖ Documentation API compl√®te
- ‚úÖ Guide migration √©crit

---

## üö® R√àGLES CRITIQUES

### ‚ùå √Ä NE JAMAIS FAIRE

1. **NE PAS** r√©√©crire tout d'un coup - it√©ratif seulement
2. **NE PAS** supprimer code avant que nouveau fonctionne
3. **NE PAS** commit sans tester
4. **NE PAS** toucher data_loader.py cette semaine (il fonctionne)
5. **NE PAS** oublier les type hints

### ‚úÖ √Ä TOUJOURS FAIRE

1. **TOUJOURS** tester avec `--test-mode` d'abord
2. **TOUJOURS** cr√©er branche Git avant changement majeur
3. **TOUJOURS** logger les d√©cisions importantes
4. **TOUJOURS** monitorer RAM pendant tests
5. **TOUJOURS** documenter les changements

---

## üìù CHECKLIST JOURNALI√àRE

### Avant de Commencer

- [ ] Git branch cr√©√©e (`git checkout -b fix/memory-aware`)
- [ ] Environment activ√© (`.toniot/bin/activate`)
- [ ] Tests existants passent (`python main_test.py`)

### Pendant le D√©veloppement

- [ ] Code √©crit avec type hints
- [ ] Docstrings ajout√©es
- [ ] Logs informatifs ajout√©s
- [ ] Exceptions sp√©cifiques utilis√©es

### Avant de Commit

- [ ] `python3 main.py --test-mode` passe
- [ ] RAM < 70% v√©rifi√©
- [ ] Logs propres (pas d'erreurs)
- [ ] Code format√© (black/autopep8)
- [ ] Git commit avec message descriptif

---

## üí° PRIORIT√â ABSOLUE: MemoryAwareProcessor

**√Ä FAIRE MAINTENANT**:

1. Cr√©er fichier `src/core/memory_manager.py`
2. Copier code MemoryAwareProcessor complet ci-dessus
3. Int√©grer dans `trainer.py` (ligne 47 et 70-74)
4. Tester: `python3 main.py --test-mode`
5. V√©rifier logs "[MemoryAware] ..." apparaissent
6. Si OK, int√©grer dans validator.py, tester.py, xai_manager.py

**Temps estim√©**: 2-3 heures
**Impact**: R√©sout probl√®me RAM critique imm√©diatement

---

## üéì R√âSUM√â EX√âCUTIF

**√âtat actuel**: Code fonctionnel mais fragile sur RAM
**Probl√®me #1**: Conversions Dask‚ÜíPandas non contr√¥l√©es ‚Üí RAM 93%
**Solution #1**: MemoryAwareProcessor (2-3h de travail)
**ROI attendu**: -50% risque OOM, +90% fiabilit√©

**Commence par l√†** ‚Üí MemoryAwareProcessor ‚Üí Teste ‚Üí Puis passe au reste

Bon courage! üöÄ
